* I think we need to put our 404 logic back in here. Let's see what happens after some runs....
------------------------------------------
Possible features?:
  * Clean up the filters code a bit and get them testing
  * And maybe we can use the typhoeus lib a bit more to better speed up the runs
    * And we might need to add that retry-404's thing again
  * FEATURE: Rather then 'search/sss?query=artist+needed', maybe use CraigScrape::search_for('artist needed')
  * FEATURE: Support for post submissions?
  * FEATURE: Stats in the email: bytes transferred, generation time, urls scrapped, posts scrapped
  * FEATURE: We might want to parse the sub-locations in the geo-location code... (so for miami: brw/mia/wpb)

------------------------------------------
# Quickie craigscrape examples/tests:
$: << './lib'
require 'libcraigscrape'

CraigScrape.new('us/fl/miami', 'us/fl/keys').posts_since(Time.now - 3600*48, 'search/sss?query=z06')
CraigScrape.new('us/fl/miami').posts('search/sss?query=z06')
i=0; CraigScrape.new('us/fl/miami', 'us/fl/keys').each_post('rea', 'search/sss?query=rack'){|p| break if i>50; puts p.inspect ;i += 1;}
CraigScrape.new('us/fl/miami', 'us/fl/keys').each_listing('rea', 'search/sss?query=rack'){ |listing| puts listing.inspect }
CraigScrape.new('us/fl/miami', 'us/fl/keys').listings('rea', 'search/sss?query=rack')


-----------------------------------------
# Quickie way to analyze an email:
$: << './lib'
require 'libcraigscrape'

email=<<EOD
email contents here
EOD
r=/regex-here/

email.tr('\n', '').scan( /http\:\/\/[^ \"]+/).reject{|l| !/\.html/.match l}.each{ |l|
  puts "\nURL: %s" % l
  c = CraigScrape::Posting.new l
  puts "  * Label : %s" % r.match(c.label).to_a[0]
  puts "  * Contents: %s" % r.match(c.contents_as_plain).to_a[0]
}

-----------------------------------------
